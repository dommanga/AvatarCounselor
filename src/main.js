import * as THREE from "three";
import { GLTFLoader } from "three/addons/loaders/GLTFLoader.js";
import { OrbitControls } from "three/addons/controls/OrbitControls.js";
import { AvatarController } from "./avatar.js";
import { SpeechRecognitionManager } from "./speech.js";
import { UIController } from "./ui.js";

// Scene setup
const scene = new THREE.Scene();
scene.background = new THREE.Color(0xf0f0f0);

// Camera
const camera = new THREE.PerspectiveCamera(
  45,
  window.innerWidth / window.innerHeight,
  0.1,
  1000
);
camera.position.set(0, 0.5, 1);

// Renderer
const renderer = new THREE.WebGLRenderer({ antialias: true });
renderer.setSize(window.innerWidth, window.innerHeight);
renderer.shadowMap.enabled = true;
document.getElementById("canvas-container").appendChild(renderer.domElement);

// Controls
const controls = new OrbitControls(camera, renderer.domElement);
controls.target.set(0, 0.5, 0);
controls.update();

// Lights
const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
scene.add(ambientLight);

const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
directionalLight.position.set(5, 10, 5);
directionalLight.castShadow = true;
scene.add(directionalLight);

// Ground
const groundGeometry = new THREE.PlaneGeometry(10, 10);
const groundMaterial = new THREE.MeshStandardMaterial({ color: 0xcccccc });
const ground = new THREE.Mesh(groundGeometry, groundMaterial);
ground.rotation.x = -Math.PI / 2;
ground.receiveShadow = true;
scene.add(ground);

// Avatar and Speech Recognition managers
const avatarController = new AvatarController();
let speechManager = null;
let uiController = null;

// Load avatar
const loader = new GLTFLoader();

loader.load(
  "./assets/avatar_torso.glb", // GLB File
  (gltf) => {
    const avatar = gltf.scene;
    scene.add(avatar);
    avatarController.init(avatar);
    console.log("âœ… Avatar loaded successfully!");

    // Initialize speech recognition after avatar loads
    initializeSpeechRecognition();
  },
  (progress) => {
    console.log(
      "Loading:",
      ((progress.loaded / progress.total) * 100).toFixed(0) + "%"
    );
  },
  (error) => {
    console.error("âŒ Error loading avatar:", error);
  }
);

// Initialize speech recognition system
function initializeSpeechRecognition() {
  console.log("ðŸŽ¤ Initializing speech recognition...");

  // Create UI
  uiController = new UIController();

  // Create speech manager
  speechManager = new SpeechRecognitionManager();

  if (!speechManager.supported) {
    alert(
      "Speech recognition is not supported in this browser. Please use Chrome."
    );
    return;
  }

  // Set up callbacks
  speechManager.onTranscriptUpdate = (finalText, interimText) => {
    uiController.updateTranscript(finalText, interimText);
  };

  speechManager.onFinalTranscript = (text) => {
    console.log("ðŸ“ Final transcript:", text);
    uiController.addToHistory(text);

    // TODO: Phase 1-4 - Send to LLM for emotion analysis
    analyzeEmotionPlaceholder(text);
  };

  speechManager.onError = (error) => {
    console.error("âŒ Speech error:", error);
    if (error === "not-allowed") {
      alert("Microphone permission denied. Please allow microphone access.");
    }
  };

  speechManager.onStatusChange = (isListening) => {
    uiController.setListeningStatus(isListening);
  };

  // Connect UI buttons
  uiController.micButton.addEventListener("click", () => {
    speechManager.toggle();
  });

  uiController.clearButton.addEventListener("click", () => {
    uiController.clearHistory();
    speechManager.clearTranscript();
  });

  uiController.languageSelect.addEventListener("change", (e) => {
    speechManager.setLanguage(e.target.value);
  });

  console.log("âœ… Speech recognition initialized!");
}

// Placeholder for emotion analysis
function analyzeEmotionPlaceholder(text) {
  console.log("ðŸ§  Analyzing emotion from:", text);

  // Simple keyword-based emotion detection (temporary)
  const lowerText = text.toLowerCase();
  let emotion = "neutral";

  if (
    lowerText.includes("í–‰ë³µ") ||
    lowerText.includes("ê¸°ì˜") ||
    lowerText.includes("ì¢‹ì•„")
  ) {
    emotion = "joy";
  } else if (
    lowerText.includes("ìŠ¬í”„") ||
    lowerText.includes("ìš°ìš¸") ||
    lowerText.includes("ìŠ¬í””")
  ) {
    emotion = "sadness";
  } else if (
    lowerText.includes("í™”") ||
    lowerText.includes("ì‹ ê²½ì§ˆ") ||
    lowerText.includes("ì§œì¦")
  ) {
    emotion = "anger";
  } else if (
    lowerText.includes("ë¬´ì„œ") ||
    lowerText.includes("ë‘ë µ") ||
    lowerText.includes("ê³µí¬")
  ) {
    emotion = "fear";
  }

  console.log("ðŸŽ­ Detected emotion:", emotion);
  avatarController.setEmotion(emotion);
}

// UI Controls
document.getElementById("btn-smile").addEventListener("click", () => {
  avatarController.setEmotion("joy");
});

document.getElementById("btn-sad").addEventListener("click", () => {
  avatarController.setEmotion("sadness");
});

document.getElementById("btn-neutral").addEventListener("click", () => {
  avatarController.setEmotion("neutral");
});

document.getElementById("smile-slider").addEventListener("input", (e) => {
  const value = parseFloat(e.target.value);
  document.getElementById("smile-value").textContent = value.toFixed(1);
  avatarController.setMorphTarget("mouthSmile", value);
});

document.getElementById("brow-slider").addEventListener("input", (e) => {
  const value = parseFloat(e.target.value);
  document.getElementById("brow-value").textContent = value.toFixed(1);
  avatarController.setMorphTarget("browInnerUp", value);
});

// Animation loop
function animate() {
  requestAnimationFrame(animate);
  controls.update();
  avatarController.update();
  renderer.render(scene, camera);
}
animate();

// Handle window resize
window.addEventListener("resize", () => {
  camera.aspect = window.innerWidth / window.innerHeight;
  camera.updateProjectionMatrix();
  renderer.setSize(window.innerWidth, window.innerHeight);
});
